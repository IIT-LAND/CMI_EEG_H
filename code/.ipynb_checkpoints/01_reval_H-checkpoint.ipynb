{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfe8c444",
   "metadata": {},
   "source": [
    "# Data-driven subtyping based on EEG derived Hurst exponent data in from CMI-HBN\n",
    "## Stability-based relative clustering validation - reval\n",
    "\n",
    "Import all needed libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e671585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nbertelsen/opt/anaconda3/lib/python3.9/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/Users/nbertelsen/opt/anaconda3/lib/python3.9/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/Users/nbertelsen/opt/anaconda3/lib/python3.9/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/Users/nbertelsen/opt/anaconda3/lib/python3.9/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "from datetime import date\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from umap import UMAP\n",
    "from reval.best_nclust_cv import FindBestClustCV\n",
    "from reval.visualization import plot_metrics\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c724f83",
   "metadata": {},
   "source": [
    "## Setting up the data before running the analysis\n",
    "\n",
    "Read in EEG preprocessed data and phenotypic data. Cut out parts that are relevant for specific parts of the analysis (i.e. resting state blocks eyes open and closed are cut and concatenated separately)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e49dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set datapath and import data\n",
    "\n",
    "# ===================================================================\n",
    "# PATHS -------------------------------------------------------------\n",
    "rootpath = \"/Users/nbertelsen/projects/cmi_eeg_rs_H\" \n",
    "data_path = \"%s/data/tidy\" % rootpath\n",
    "\n",
    "# ===================================================================\n",
    "# Read in data ------------------------------------------------------\n",
    "pheno_fn = os.path.join(data_path,\"tidy_pheno.csv\")\n",
    "hurst_fn = os.path.join(data_path, \"tidy_H.csv\")\n",
    "pheno_df = pd.read_csv(pheno_fn, index_col = False)\n",
    "hurst_df = pd.read_csv(hurst_fn, index_col = False)\n",
    "\n",
    "# ===================================================================\n",
    "# Merge and subset data ---------------------------------------------\n",
    "\n",
    "# merge datasets\n",
    "df_merged = pd.merge(pheno_df, hurst_df)\n",
    "df_clean = df_merged.loc[df_merged['exclude'] == 'No', :]\n",
    "df_clean = df_clean.loc[df_clean['dx1_long'] != 'No Diagnosis Given: Incomplete Eval', :]\n",
    "df_clean = df_clean.drop(columns = 'Unnamed: 0') # this column is duplicated for some reason\n",
    "df_clean = df_clean.drop_duplicates()\n",
    "\n",
    "df_asd = df_clean.loc[df_clean['dx'] != 'TD', :]\n",
    "df_asd = df_asd.loc[df_asd['dx'] != 'ADHD', :]\n",
    "df_asd = df_asd.loc[df_asd['dx'] != 'LANG', :]\n",
    "df_asd = df_asd.loc[df_asd['dx'] != 'ID', :]\n",
    "df_asd = df_asd.loc[df_asd['dx'] != 'ADHD+LANG', :]\n",
    "df_asd = df_asd.loc[df_asd['dx'] != 'ADHD+ID', :]\n",
    "df_asd = df_asd.loc[df_asd['dx'] != 'LANG+ID', :]\n",
    "df_asd = df_asd.loc[df_asd['dx'] != 'ADHD+LANG+ID', :]\n",
    "df_asd_males = df_asd.loc[df_asd['sex'] != 'Female', :]\n",
    "df_asd_males_relabeled = df_asd_males\n",
    "\n",
    "# relabel into two groups (ASD and ASD+ADHD)\n",
    "df_asd_males_relabeled.loc[((df_asd_males['dx'] == 'ASD+ADHD+LANG') | \n",
    "             (df_asd_males['dx'] == 'ASD+ADHD+ID') | \n",
    "             (df_asd_males['dx'] == 'ASD+ADHD+LANG+ID')), 'dx'] =  'ASD+ADHD'\n",
    "\n",
    "df_asd_males_relabeled.loc[((df_asd_males['dx'] == 'ASD+LANG') | \n",
    "             (df_asd_males['dx'] == 'ASD+ID') | \n",
    "             (df_asd_males['dx'] == 'ASD+LANG+ID')), 'dx'] =  'ASD'\n",
    "\n",
    "# divide into tasks\n",
    "\n",
    "# Resting State Eyes Open\n",
    "df_rseo = df_asd_males_relabeled.loc[(df_asd_males_relabeled['task'] == 'RestingState') & (df_clean['condition'] == 'open'), :]\n",
    "df_rseo_b1 = df_rseo.loc[(df_rseo['block'] == 1), :]\n",
    "df_rseo_b2 = df_rseo.loc[(df_rseo['block'] == 2), :]\n",
    "df_rseo_b3 = df_rseo.loc[(df_rseo['block'] == 3), :]\n",
    "df_rseo_b4 = df_rseo.loc[(df_rseo['block'] == 4), :]\n",
    "df_rseo_b5 = df_rseo.loc[(df_rseo['block'] == 5), :]\n",
    "\n",
    "tmp_df_rseo_b1 = df_rseo_b1.pivot(index = 'subid', columns = 'Electrodes', values = 'H')\n",
    "new_chan_names = []\n",
    "chan_names = tmp_df_rseo_b1.columns\n",
    "for channel in chan_names:\n",
    "    new_chan_names.append(\"%s_b1\" % channel)\n",
    "new_chan_names\n",
    "tmp_df_rseo_b1.columns = new_chan_names\n",
    "\n",
    "tmp_df_rseo_b2 = df_rseo_b2.pivot(index = 'subid', columns = 'Electrodes', values = 'H')\n",
    "new_chan_names = []\n",
    "chan_names = tmp_df_rseo_b2.columns\n",
    "for channel in chan_names:\n",
    "    new_chan_names.append(\"%s_b2\" % channel)\n",
    "new_chan_names\n",
    "tmp_df_rseo_b2.columns = new_chan_names\n",
    "\n",
    "tmp_df_rseo_b3 = df_rseo_b3.pivot(index = 'subid', columns = 'Electrodes', values = 'H')\n",
    "new_chan_names = []\n",
    "chan_names = tmp_df_rseo_b3.columns\n",
    "for channel in chan_names:\n",
    "    new_chan_names.append(\"%s_b3\" % channel)\n",
    "new_chan_names\n",
    "tmp_df_rseo_b3.columns = new_chan_names\n",
    "\n",
    "tmp_df_rseo_b4 = df_rseo_b4.pivot(index = 'subid', columns = 'Electrodes', values = 'H')\n",
    "new_chan_names = []\n",
    "chan_names = tmp_df_rseo_b4.columns\n",
    "for channel in chan_names:\n",
    "    new_chan_names.append(\"%s_b4\" % channel)\n",
    "new_chan_names\n",
    "tmp_df_rseo_b4.columns = new_chan_names\n",
    "\n",
    "tmp_df_rseo_b5 = df_rseo_b5.pivot(index = 'subid', columns = 'Electrodes', values = 'H')\n",
    "new_chan_names = []\n",
    "chan_names = tmp_df_rseo_b5.columns\n",
    "for channel in chan_names:\n",
    "    new_chan_names.append(\"%s_b5\" % channel)\n",
    "new_chan_names\n",
    "tmp_df_rseo_b5.columns = new_chan_names\n",
    "\n",
    "# combine blocks for rest open\n",
    "tmp_df_rseo = [tmp_df_rseo_b1, tmp_df_rseo_b2, tmp_df_rseo_b3, tmp_df_rseo_b4, tmp_df_rseo_b5]\n",
    "df_rseo_allconcat = pd.concat(tmp_df_rseo, axis=1)\n",
    "\n",
    "# Resting State Eyes Closed\n",
    "df_rsec = df_asd_males_relabeled.loc[(df_asd_males_relabeled['task'] == 'RestingState') & (df_clean['condition'] == 'closed'), :]\n",
    "df_rsec_b1 = df_rsec.loc[(df_rsec['block'] == 1), :]\n",
    "df_rsec_b2 = df_rsec.loc[(df_rsec['block'] == 2), :]\n",
    "df_rsec_b3 = df_rsec.loc[(df_rsec['block'] == 3), :]\n",
    "df_rsec_b4 = df_rsec.loc[(df_rsec['block'] == 4), :]\n",
    "df_rsec_b5 = df_rsec.loc[(df_rsec['block'] == 5), :]\n",
    "\n",
    "tmp_df_rsec_b1 = df_rsec_b1.pivot(index = 'subid', columns = 'Electrodes', values = 'H')\n",
    "new_chan_names = []\n",
    "chan_names = tmp_df_rsec_b1.columns\n",
    "for channel in chan_names:\n",
    "    new_chan_names.append(\"%s_b1\" % channel)\n",
    "new_chan_names\n",
    "tmp_df_rsec_b1.columns = new_chan_names\n",
    "\n",
    "tmp_df_rsec_b2 = df_rsec_b2.pivot(index = 'subid', columns = 'Electrodes', values = 'H')\n",
    "new_chan_names = []\n",
    "chan_names = tmp_df_rsec_b2.columns\n",
    "for channel in chan_names:\n",
    "    new_chan_names.append(\"%s_b2\" % channel)\n",
    "new_chan_names\n",
    "tmp_df_rsec_b2.columns = new_chan_names\n",
    "\n",
    "tmp_df_rsec_b3 = df_rsec_b3.pivot(index = 'subid', columns = 'Electrodes', values = 'H')\n",
    "new_chan_names = []\n",
    "chan_names = tmp_df_rsec_b3.columns\n",
    "for channel in chan_names:\n",
    "    new_chan_names.append(\"%s_b3\" % channel)\n",
    "new_chan_names\n",
    "tmp_df_rsec_b3.columns = new_chan_names\n",
    "\n",
    "tmp_df_rsec_b4 = df_rsec_b4.pivot(index = 'subid', columns = 'Electrodes', values = 'H')\n",
    "new_chan_names = []\n",
    "chan_names = tmp_df_rsec_b4.columns\n",
    "for channel in chan_names:\n",
    "    new_chan_names.append(\"%s_b4\" % channel)\n",
    "new_chan_names\n",
    "tmp_df_rsec_b4.columns = new_chan_names\n",
    "\n",
    "tmp_df_rsec_b5 = df_rsec_b5.pivot(index = 'subid', columns = 'Electrodes', values = 'H')\n",
    "new_chan_names = []\n",
    "chan_names = tmp_df_rsec_b5.columns\n",
    "for channel in chan_names:\n",
    "    new_chan_names.append(\"%s_b5\" % channel)\n",
    "new_chan_names\n",
    "tmp_df_rsec_b5.columns = new_chan_names\n",
    "\n",
    "# combine blocks for rest closed\n",
    "tmp_df_rsec = [tmp_df_rsec_b1, tmp_df_rsec_b2, tmp_df_rsec_b3, tmp_df_rsec_b4, tmp_df_rsec_b5]\n",
    "df_rsec_allconcat = pd.concat(tmp_df_rsec, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304d2619",
   "metadata": {},
   "source": [
    "## Wrap up the reval analysis pipeline into a repeatable function\n",
    "\n",
    "This function wraps together the various steps necessary to run reval. This allows for running reval repeatedly on different types of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17744613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function input parameters: \n",
    "# data -  \n",
    "# data_name - \n",
    "# tidy_data - \n",
    "# classifier2use -\n",
    "# clustering2use - \n",
    "# n_centile_split - \n",
    "# validation_set_percentage - \n",
    "# preproc - if set to \"True\"\n",
    "# dim_reduction - \n",
    "# umap_neighbors - \n",
    "# umap_ncomponents -\n",
    "# umap_min_dist -\n",
    "# pca_ncomponents -\n",
    "# do_dim_reduct_plot -\n",
    "# nrand_runs -\n",
    "# n_iter_cv -\n",
    "# n_folds2use -\n",
    "# n_clusters_range -\n",
    "# custom_ncl2use -\n",
    "\n",
    "def run_reval(data, \n",
    "              data_name, \n",
    "              tidy_data = None,\n",
    "              classifier2use = \"KNN\",\n",
    "              clustering2use = \"Kmeans\",\n",
    "              n_centile_split = 2, \n",
    "              validation_set_percentage = 0.5, \n",
    "              preproc = True,\n",
    "              dim_reduction = \"umap\",\n",
    "              umap_neighbors = 30,\n",
    "              umap_ncomponents = 2,\n",
    "              umap_min_dist = 0.0,\n",
    "              pca_ncomponents = 2,\n",
    "              do_dim_reduct_plot = True,\n",
    "              nrand_runs = 100,\n",
    "              n_iter_cv = 100,\n",
    "              n_folds2use = 2,\n",
    "              n_clusters_range = list(range(2,6,1)),\n",
    "              custom_ncl2use = None):\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Dataset:  %s\" % data_name)\n",
    "    \n",
    "    #============================================================================\n",
    "    # STEP 1: Setting up the data -----------------------------------------------\n",
    "    \n",
    "    # this if statement uses tidy_data if its not None. \n",
    "    \n",
    "    if tidy_data is not None:\n",
    "        df = tidy_data\n",
    "        df_strat = data[['subid', 'dx', 'sex', 'age']].drop_duplicates()\n",
    "        df_strat.set_index('subid', inplace = True)\n",
    "    else:\n",
    "        df = data.pivot(index = 'subid', columns = 'Electrodes', values = 'H')\n",
    "        df_strat = data[['subid', 'dx', 'sex', 'age']].drop_duplicates()\n",
    "        df_strat.set_index('subid', inplace = True)\n",
    "    \n",
    "    # Cut up age into groups.\n",
    "    # We cut age into groups for the purposes of matching. Ideally we'd want every subject to be \n",
    "    # pair-wise matched. However, in smaller datasets where there might not be a matching count\n",
    "    # of individuals per each grouping, we would get an error. \n",
    "    \n",
    "    print('Train-Validation Split:  Splitting into training and validation sets for reval.')\n",
    "    print('Train-Validation Split:  Attempting to balance factors such as diagnosis, sex, and age group')\n",
    "    print('Train-Validation Split:  Making an age grouping based on %f percentage centile splits of age.' % (1/n_centile_split))\n",
    "    print('Train-Validation Split:  This will be used when stratifying into training and validation.')\n",
    "    print('Train-Validation Split:  The split between training and validation was %0.2f:%0.2f' % (1-validation_set_percentage,\n",
    "                                                                                                  validation_set_percentage))\n",
    "\n",
    "    df_strat['age_group'] = pd.qcut(df_strat['age'], n_centile_split, labels=False)\n",
    "    df_strat['age_group'] = df_strat['age_group'].astype(str)\n",
    "    \n",
    "    # Make the stratification vector that includes Dx, Sex, and Age grouping.\n",
    "    strat_vec = df_strat['dx'] + df_strat['age_group']\n",
    "    \n",
    "    # Split into the initial training and validation sets for reval\n",
    "    X_tr_split, X_ts_split = train_test_split(df,\n",
    "                                              stratify = strat_vec,\n",
    "                                              test_size = validation_set_percentage,\n",
    "                                              random_state = 22)\n",
    "    \n",
    "    # grab indices after the split\n",
    "    X_tr_indx = X_tr_split.index\n",
    "    X_ts_indx = X_ts_split.index\n",
    "    \n",
    "    # grab stratification vector data for the training and validation split\n",
    "    strat_vec_train = strat_vec[X_tr_indx]\n",
    "    strat_vec_validation = strat_vec[X_ts_indx]\n",
    "    \n",
    "    # print the number of individuals in the training and validation sets\n",
    "    print('Train-Validation Split:  There are ',X_tr_split.shape[0],' subjects in the train set')\n",
    "    print('Train-Validation Split:  There are ',X_ts_split.shape[0],' subjects in the validation set')\n",
    "    print(\" \")\n",
    "    #============================================================================\n",
    "    \n",
    "    \n",
    "    \n",
    "    #============================================================================\n",
    "    # STEP 2: Preprocessing the data --------------------------------------------\n",
    "        \n",
    "    # Preprocessing \n",
    "    if preproc:\n",
    "\n",
    "        if dim_reduction==\"pca\":\n",
    "            ncomps2use = pca_ncomponents\n",
    "            dim_reduct_name = \"PCA\"\n",
    "        elif dim_reduction==\"umap\":\n",
    "            ncomps2use = umap_ncomponents\n",
    "            dim_reduct_name = \"UMAP\"\n",
    "\n",
    "        # # imputation\n",
    "        # impute = KNNImputer(n_neighbors=5)\n",
    "        \n",
    "        # scaling - z-scoring each channel\n",
    "        Scaler = StandardScaler()\n",
    "        \n",
    "        print(\"Preproc:  Running preprocessing - scaling mean 0, SD 1, and %s dimensionality reduction\" % dim_reduct_name)\n",
    "        print(\"Preproc:  Compressed data to %d %s components\" % (ncomps2use, dim_reduct_name))\n",
    "        print(\" \")\n",
    "\n",
    "        if dim_reduction == \"pca\":\n",
    "            \n",
    "            # PCA - dimensionality reduction\n",
    "            pca = PCA(n_components = pca_ncomponents)\n",
    "\n",
    "            # apply all transformations - both scaling and PCA\n",
    "            X_tr_prepr = pd.DataFrame(pca.fit_transform(Scaler.fit_transform(X_tr_split)), \n",
    "                                      index = X_tr_split.index)\n",
    "            X_ts_prepr = pd.DataFrame(pca.transform(Scaler.transform(X_ts_split)), \n",
    "                                      index = X_ts_split.index)\n",
    "            \n",
    "        elif dim_reduction == \"umap\":\n",
    "            \n",
    "            # UMAP - dimensionality reduction\n",
    "            Umap = UMAP(n_neighbors = umap_neighbors,\n",
    "                        min_dist = umap_min_dist,\n",
    "                        n_components = umap_ncomponents,\n",
    "                        random_state = 24)\n",
    "\n",
    "            # apply all transformations - both scaling and UMAP\n",
    "            X_tr_prepr = pd.DataFrame(Umap.fit_transform(Scaler.fit_transform(X_tr_split)),\n",
    "                                      index = X_tr_split.index)\n",
    "\n",
    "            X_ts_prepr = pd.DataFrame(Umap.transform(Scaler.transform(X_ts_split)),\n",
    "                                      index = X_ts_split.index)\n",
    "        \n",
    "        # specify the data that reval will use later\n",
    "        X_tr_data4reval = X_tr_prepr\n",
    "        X_ts_data4reval = X_ts_prepr\n",
    "    else:\n",
    "        # When no preprocessing is done, it just takes the raw data after the split\n",
    "        X_tr_data4reval = X_tr_split\n",
    "        X_ts_data4reval = X_ts_split\n",
    "    #============================================================================\n",
    "\n",
    "\n",
    "    #============================================================================\n",
    "    # Step 3: Run reval ---------------------------------------------------------\n",
    "    \n",
    "    # Run reval\n",
    "    print('reval:  Running the main reval algorithm...')\n",
    "    print(\" \")\n",
    "\n",
    "    print('reval PARAMETERS:  Using %s as the classifier and %s as the clustering algorithm' % (classifier2use,\n",
    "                                                                                                clustering2use))\n",
    "    print(\"reval PARAMETERS:  Number of random runs in the internal split = %d\" % nrand_runs)\n",
    "    print(\"reval PARAMETERS:  CV scheme for internal loop is k=%d\" % n_folds2use)\n",
    "    print(\"reval PARAMETERS:  Number of times to iterate cross validation is %d\" % n_iter_cv)\n",
    "    print(\" \")\n",
    "        \n",
    "    # Specify classifcation algorithm\n",
    "    num_neigh = 3\n",
    "    clf = KNeighborsClassifier(n_neighbors = num_neigh)      #KNN  \n",
    "    \n",
    "    # Specify cluster algorithm\n",
    "    clust = KMeans(random_state = 42)                        #Kmeans\n",
    "    \n",
    "    # initialize FindBestClustCV\n",
    "    # First step of reval: training data is\n",
    "    # sent through an internal cross validation loop in order to \n",
    "    # find the optimal k that produces minimal stability (e.g., \n",
    "    # achieves good cross validated accuracy).\n",
    "    relval = FindBestClustCV(s = clf,\n",
    "                             c = clust, \n",
    "                             nfold = n_folds2use, \n",
    "                             nclust_range = n_clusters_range,\n",
    "                             nrand = nrand_runs)\n",
    "    \n",
    "    # Train reval\n",
    "    # This is done to find the best k, which will be denoted as the ncl output\n",
    "    metric, ncl = relval.best_nclust(X_tr_data4reval,\n",
    "                                     iter_cv = n_iter_cv,\n",
    "                                     strat_vect = strat_vec_train)\n",
    "    \n",
    "    # Apply reval to the validation set\n",
    "    # This trains a classifier based on the training data and applies it to the \n",
    "    # validation set to see if it can predict the validation set's clustering labels.\n",
    "    if custom_ncl2use is not None:\n",
    "        ncl2use = custom_ncl2use\n",
    "    else:\n",
    "        ncl2use = ncl\n",
    "    \n",
    "    out = relval.evaluate(X_tr_data4reval,\n",
    "                          X_ts_data4reval,\n",
    "                          ncl2use) \n",
    "    \n",
    "    print(metric)\n",
    "    print(\"reval RESULTS:  The best number of clusters that minimizes normalized stability is %d\" % ncl2use)\n",
    "    print(\"reval RESULTS:  Accuracy on training set = %f\" % out.train_acc)\n",
    "    print(\"reval RESULTS:  Accuracy on validation set= %f\" % out.test_acc)\n",
    "    print(\" \")\n",
    "\n",
    "\n",
    "    logging.info(f\"Training ACC: {out.train_acc}, Test ACC: {out.test_acc}\")\n",
    "    \n",
    "    #============================================================================\n",
    "    # Step 4: Plotting the results ----------------------------------------------\n",
    "\n",
    "    # PLOTTING ===========================\n",
    "    # plot normalized stability, the k with the lowest stability is the best k\n",
    "    plot_metrics(metric)\n",
    "    \n",
    "    # UMAP or PCA plot of training set with cluster labels shown\n",
    "    colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red','tab:purple',\n",
    "              'tab:brown', 'tab:pink', 'tab:gray','tab:olive', 'tab:cyan']\n",
    "        \n",
    "    if do_dim_reduct_plot:\n",
    "        \n",
    "        if ncomps2use==2:\n",
    "            \n",
    "            fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10,6))\n",
    "            ax1.scatter(X_tr_data4reval[[0]],\n",
    "                        X_tr_data4reval[[1]],\n",
    "                        color=[colors[lab] for lab in out.train_cllab])\n",
    "            ax2.scatter(X_ts_data4reval[[0]],\n",
    "                        X_ts_data4reval[[1]],\n",
    "                        color=[colors[lab] for lab in out.test_cllab])\n",
    "            \n",
    "        elif ncomps2use>2:\n",
    "            \n",
    "            fig = plt.figure(figsize=(10,6))\n",
    "            \n",
    "            ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "            ax1.scatter(xs = X_tr_data4reval[[0]], \n",
    "                        ys = X_tr_data4reval[[1]], \n",
    "                        zs = X_tr_data4reval[[2]],\n",
    "                        c = [colors[lab] for lab in out.train_cllab])\n",
    "        \n",
    "            ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "            ax2.scatter(xs = X_ts_data4reval[[0]], \n",
    "                        ys = X_ts_data4reval[[1]], \n",
    "                        zs = X_ts_data4reval[[2]],\n",
    "                        c = [colors[lab] for lab in out.test_cllab])\n",
    "        \n",
    "        ax1.set_title('%s Training' % dim_reduct_name)\n",
    "        ax2.set_title('%s Validation' % dim_reduct_name)\n",
    "            \n",
    "    # PLOTTING ===========================    \n",
    "    #============================================================================\n",
    "    \n",
    "    \n",
    "    #============================================================================\n",
    "    # Step 5:  Saving the results -----------------------------------------------\n",
    "    \n",
    "    # get the outputs clustering labels\n",
    "    X_ts_split['cl'] = out.test_cllab\n",
    "    X_tr_split['cl'] = out.train_cllab    \n",
    "    X_tr_labels = X_tr_split['cl']\n",
    "    X_ts_labels = X_ts_split['cl']\n",
    "    \n",
    "    # results dictionary to pack everything into\n",
    "    result_dict = {'classifier':classifier2use,\n",
    "                   'clustering':clustering2use,\n",
    "                   'preproc':preproc,\n",
    "                   'validation_set_percentage':validation_set_percentage,\n",
    "                   'n_age_centile_splits':n_centile_split,\n",
    "                   'umap_neighbors':umap_neighbors,\n",
    "                   'umap_ncomponents':umap_ncomponents,\n",
    "                   'reval_nrand_runs':nrand_runs,\n",
    "                   'reval_n_cv_iter':n_iter_cv,\n",
    "                   'reval_internal_kfold':n_folds2use,\n",
    "                   'reval_krange':n_clusters_range,\n",
    "                   'reval_data_training':X_tr_data4reval,\n",
    "                   'reval_data_validation':X_ts_data4reval,\n",
    "                   'result_metric':metric,\n",
    "                   'result_best_k':ncl,\n",
    "                   'result_training_acc':out.train_acc,\n",
    "                   'result_validation_acc':out.test_acc,\n",
    "                   'result_out_train_cllab':out.train_cllab,\n",
    "                   'result_out_validation_cllab':out.test_cllab,                 \n",
    "                   'result_training_labels':X_tr_labels,\n",
    "                   'result_validation_labels':X_ts_labels\n",
    "                  }\n",
    "    #============================================================================\n",
    "    \n",
    "    print(\"Finished!\")\n",
    "    return(result_dict)\n",
    "\n",
    "# function to relabel subtypes\n",
    "def relabel_subtypes(labels, newlabels):\n",
    "    \n",
    "    # orignal color ordering\n",
    "    # blue = 1\n",
    "    # orange = 2\n",
    "    # green = 3\n",
    "    \n",
    "    unique_labels = labels.unique()\n",
    "    unique_labels = np.sort(unique_labels)\n",
    "    \n",
    "    out_labels = labels.copy()\n",
    "\n",
    "    for idx, label in enumerate(unique_labels):\n",
    "        mask = labels==label\n",
    "        out_labels[mask] = newlabels[idx]\n",
    "        \n",
    "    out_labels = pd.Series(out_labels)\n",
    "    return(out_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79544079",
   "metadata": {},
   "source": [
    "## Main analyses\n",
    "Running the reval function on resting state eyes open and eyes closed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a990d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking for more global structure (higher umap_neighbors)\n",
    "restopen_allblocks_res_global = run_reval(data = df_rseo_b1, \n",
    "                                          tidy_data = df_rseo_allconcat, \n",
    "                                          data_name = \"RestOpen ALL BLOCKS - Global\",\n",
    "                                          validation_set_percentage = 0.5,\n",
    "                                          n_centile_split = 10, \n",
    "                                          dim_reduction=\"umap\",\n",
    "                                          umap_neighbors = 30, umap_ncomponents = 3, umap_min_dist = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253bc72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking for more global structure (higher umap_neighbors)\n",
    "restclosed_allblocks_res_global = run_reval(data = df_rsec_b1,\n",
    "                                            tidy_data = df_rsec_allconcat,\n",
    "                                            data_name = \"RestClosed ALL BLOCKS - Global\",\n",
    "                                            validation_set_percentage = 0.5,\n",
    "                                            n_centile_split = 10,\n",
    "                                            dim_reduction=\"umap\",\n",
    "                                            umap_neighbors = 30, umap_ncomponents = 3, umap_min_dist = 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a339bff3",
   "metadata": {},
   "source": [
    "Now we will conclude by putting the subtype labels together with the phenotypic data and then outputting it to a csv file for further downstream analyses (e.g., testing if the subtypes differ on phenotypic variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6c6d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put together the subgroup labels with the pheno data and then write out to a file\n",
    "\n",
    "# put together the training and validation labels into one PD series\n",
    "rsopen_subgroup_glob = pd.concat([restopen_allblocks_res_global['result_training_labels'], restopen_allblocks_res_global['result_validation_labels']])+1\n",
    "rsclosed_subgroup_glob = pd.concat([restclosed_allblocks_res_global['result_training_labels'], restclosed_allblocks_res_global['result_validation_labels']])+1\n",
    "\n",
    "RELABEL = True\n",
    "if RELABEL:\n",
    "    rsopen_subgroup_glob = relabel_subtypes(labels = rsopen_subgroup_glob, newlabels = [2,1])\n",
    "    rsclosed_subgroup_glob = relabel_subtypes(labels = rsclosed_subgroup_glob, newlabels = [2,1])\n",
    "\n",
    "rs_dict_glob = {\"rsclosed_subgroup_glob\": rsclosed_subgroup_glob,\n",
    "                \"rsopen_subgroup_glob\": rsopen_subgroup_glob}\n",
    "rs_labels_glob = pd.concat(rs_dict_glob, axis = 1)\n",
    "\n",
    "# left join the subgroup labels with the ASD pheno data, and the larger pheno_data file with everyone\n",
    "df_merged2 = pd.merge(df_clean, rs_labels_glob, on='subid', how='left')\n",
    "\n",
    "# get todays date\n",
    "today = date.today()\n",
    "td = today.strftime(\"%d.%m.%Y\")\n",
    "\n",
    "# write out new csv files with subgroup labels\n",
    "#df_merged2.to_csv(\"%s/tidy_H+pheno+revalsubgroups.csv\" % data_path)\n",
    "df_merged2.to_csv(os.path.join(data_path, \"tidy_H_reval_asd_males_revalsubgroups_global_\" + td + \".csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5db4c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write and save dataframes with subids and cluster labels\n",
    "\n",
    "# rs eyes open dataframe with umap1, umap2, dataset and cluster subtype\n",
    "rseo_train_col = pd.DataFrame(np.repeat('training', len(restopen_allblocks_res_global['reval_data_training'])))\n",
    "rseo_train = pd.concat([restopen_allblocks_res_global['reval_data_training'].reset_index(), \n",
    "           rseo_train_col, restopen_allblocks_res_global['result_training_labels'].reset_index(drop = True)+1], axis = 1)\n",
    "rseo_train.columns = ['subid', 'umap1', 'umap2', 'dataset','subtype']\n",
    "rseo_valid_col = pd.DataFrame(np.repeat('validation', len(restopen_allblocks_res_global['reval_data_validation'])))\n",
    "rseo_valid = pd.concat([restopen_allblocks_res_global['reval_data_validation'].reset_index(), \n",
    "           rseo_valid_col, restopen_allblocks_res_global['result_validation_labels'].reset_index(drop = True)+1], axis = 1)\n",
    "rseo_valid.columns = ['subid', 'umap1', 'umap2', 'dataset','subtype']\n",
    "rseo_all = pd.concat([rseo_train, rseo_valid], axis = 0)\n",
    "# change labels and save\n",
    "rseo_all['subtype'] = relabel_subtypes(labels = rseo_all['subtype'], newlabels = [2,1])\n",
    "rseo_all.to_csv(os.path.join(data_path, \"tidy_H_reval_asd_males_umap+cl_rsopen_global_\" + td + \".csv\"))\n",
    "\n",
    "# rs eyes closed dataframe with umap1, umap2, dataset and cluster subtype\n",
    "rsec_train_col = pd.DataFrame(np.repeat('training', len(restclosed_allblocks_res_global['reval_data_training'])))\n",
    "rsec_train = pd.concat([restclosed_allblocks_res_global['reval_data_training'].reset_index(), \n",
    "           rsec_train_col, restclosed_allblocks_res_global['result_training_labels'].reset_index(drop = True)+1], axis = 1)\n",
    "rsec_train.columns = ['subid', 'umap1', 'umap2', 'dataset','subtype']\n",
    "rsec_valid_col = pd.DataFrame(np.repeat('validation', len(restclosed_allblocks_res_global['reval_data_validation'])))\n",
    "rsec_valid = pd.concat([restclosed_allblocks_res_global['reval_data_validation'].reset_index(), \n",
    "           rsec_valid_col, restclosed_allblocks_res_global['result_validation_labels'].reset_index(drop = True)+1], axis = 1)\n",
    "rsec_valid.columns = ['subid', 'umap1', 'umap2', 'dataset','subtype']\n",
    "rsec_all = pd.concat([rsec_train, rsec_valid], axis = 0)\n",
    "# change labels and save\n",
    "rsec_all['subtype'] = relabel_subtypes(labels = rsec_all['subtype'], newlabels = [2,1])\n",
    "rsec_all.to_csv(os.path.join(data_path, \"tidy_H_reval_asd_males_umap+cl_rsclosed_global_\" + td + \".csv\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
